{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i crawl.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Ryan Barrett](https://snarfed.org/) did something similar for [IndieWeb Summit2017](https://2017.indieweb.org/) called [IndieMap](https://indiemap.org/). It's excellent, [weighing in at](https://indiemap.org/docs.html#crawl-data),\n",
    "\n",
    "> 2300 sites, 5.7M pages, 380GB HTML\n",
    "\n",
    "of crawl data. I'd like to create an always on crawler service, so my use case is slightly different. (Plus, tbh, I'm learning elastic search, so this is a two birds type scenario.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `indie_crawl` Elastic Search Index\n",
    "\n",
    "\n",
    "This will house the raw crawl data. Ryan's crawl (also accessible via [WARC files](https://indiemap.org/docs.html#crawl-data)) lists a [schema](https://indiemap.org/docs.html#schema-pages) that I adapt, anticipating later possible collaboration or enrichment. I'll have to backfill the headers once I start populating to get a better sense of what exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_indie_crawl_index()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
